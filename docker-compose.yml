version: '3.8'
services:
  web:
    build: .
    image: yeling-ai-backend:latest
    restart: always
    environment:
      - PYTHONUNBUFFERED=1
    ports:
      - "5000:5000"
    # If Ollama runs on host and you need direct access, consider uncommenting network_mode
    # network_mode: "host"
    volumes:
      - ./:/app

  nginx:
    image: nginx:stable
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./deployment/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - web
